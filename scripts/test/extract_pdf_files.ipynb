{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Downloading Congressional documents about the endangered species act from a section of congressional Record of Daily Digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import service as chromeservice\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 207 PDFs to /Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\n"
     ]
    }
   ],
   "source": [
    "#Extracting pdf files for page 1\n",
    "from selenium import webdriver # Selenium is used for browser automation.\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # ChromeDriverManager automatically downloads the right version of ChromeDriver.\n",
    "from selenium.webdriver.common.by import By  # By is a locator strategy for finding elements (e.g., by tag, class, XPath).\n",
    "import time  # time is used to pause between downloads.\n",
    "import os\n",
    "import urllib.parse  # os and urllib.parse help with file paths and URL handling.\n",
    "import re\n",
    "\n",
    "\n",
    "# Setting my download path\n",
    "download_dir = \"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\"  #folder to put the downloads\n",
    "\n",
    "# function to wait for downloads to finish\n",
    "def wait_for_downloads(download_path, timeout=30):\n",
    "    seconds = 0\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        if seconds > timeout:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,    #Ensuring that Chrome doesn't try to preview PDFs but downloads them directly.\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=ChromeService(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "# URL to fetch\n",
    "#This opens the Congress.gov search results page for the keyword \"Endagered Species Act\" in the Congressional Record, maximizes the browser, and waits up to 5 seconds for elements to load.\n",
    "\n",
    "URL = 'https://www.congress.gov/quick-search/congressional-record?wordsPhrases=crArticleContent%3A%22endangered+species+act%22&congresses%5B0%5D=all&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionDailyDigest=on&representative%5B0%5D=&senator%5B0%5D=&pageSort=relevancy&pageSize=250#' #lik with the first 250 pdfs\n",
    "\n",
    "\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "pdf_links = driver.find_elements(By.XPATH, \"//a[translate(substring(@href, string-length(@href)-3), 'PDF', 'pdf')='.pdf']\")\n",
    "\n",
    "\n",
    "base_url = driver.current_url\n",
    "downloaded = 0\n",
    "\n",
    "\n",
    "\n",
    "for link in pdf_links:\n",
    "    href = link.get_attribute('href')\n",
    "    if not href:\n",
    "        continue\n",
    "    # Resolve relative URLs\n",
    "    href_abs = urllib.parse.urljoin(base_url, href)\n",
    "    driver.get(href_abs)    #Navigating to the PDF link to trigger Chrome‚Äôs auto-download.\n",
    "    downloaded += 1\n",
    "    if downloaded >=250:   #Stops after downloading 250 files.\n",
    "            break\n",
    "    time.sleep(7) \n",
    "\n",
    "print(f\"Downloaded {downloaded} PDFs to {download_dir}\")  #Printing the number of PDFs downloaded and closes the browser.\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 117 PDFs to /Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\n"
     ]
    }
   ],
   "source": [
    "#Extracting pdf fiels for page 2\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager  \n",
    "from selenium.webdriver.common.by import By \n",
    "import time  \n",
    "import os\n",
    "import urllib.parse  \n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "download_dir = \"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\"  \n",
    "\n",
    "\n",
    "def wait_for_downloads(download_path, timeout=30):\n",
    "    seconds = 0\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        if seconds > timeout:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,   \n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=ChromeService(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "URL = 'https://www.congress.gov/quick-search/congressional-record?wordsPhrases=crArticleContent%3A%22endangered+species+act%22&congresses%5B0%5D=all&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionDailyDigest=on&representative%5B0%5D=&senator%5B0%5D=&pageSort=relevancy&pageSize=250&page=2' #lik with the first 250 pdfs\n",
    "\n",
    "\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "pdf_links = driver.find_elements(By.XPATH, \"//a[translate(substring(@href, string-length(@href)-3), 'PDF', 'pdf')='.pdf']\")\n",
    "\n",
    "\n",
    "base_url = driver.current_url\n",
    "downloaded = 0\n",
    "\n",
    "\n",
    "\n",
    "for link in pdf_links:\n",
    "    href = link.get_attribute('href')\n",
    "    if not href:\n",
    "        continue\n",
    "    \n",
    "    href_abs = urllib.parse.urljoin(base_url, href)\n",
    "    driver.get(href_abs)    \n",
    "    downloaded += 1\n",
    "    if downloaded >=250:   \n",
    "            break\n",
    "    time.sleep(7) \n",
    "\n",
    "print(f\"Downloaded {downloaded} PDFs to {download_dir}\") \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting pdf fiels for page 3\n",
    "from selenium import webdriver # Selenium is used for browser automation.\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # ChromeDriverManager automatically downloads the right version of ChromeDriver.\n",
    "from selenium.webdriver.common.by import By  # By is a locator strategy for finding elements (e.g., by tag, class, XPath).\n",
    "import time  # time is used to pause between downloads.\n",
    "import os\n",
    "import urllib.parse  # os and urllib.parse help with file paths and URL handling.\n",
    "import re\n",
    "\n",
    "\n",
    "# Set your download path\n",
    "download_dir = \"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\"  #folder to put the downloads\n",
    "\n",
    "# Define function to wait for downloads to finish\n",
    "def wait_for_downloads(download_path, timeout=30):\n",
    "    seconds = 0\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        if seconds > timeout:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,    #Ensures that Chrome doesn't try to preview PDFs but downloads them directly.\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=ChromeService(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "# URL to fetch\n",
    "#This opens the Congress.gov search results page for the keyword \"Endagered Species Act\" in the Congressional Record, maximizes the browser, and waits up to 5 seconds for elements to load.\n",
    "\n",
    "URL = 'https://www.congress.gov/quick-search/congressional-record?wordsPhrases=crArticleContent%3A%22endangered+species+act%22&congresses%5B0%5D=all&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionDailyDigest=on&representative%5B0%5D=&senator%5B0%5D=&pageSort=relevancy&pageSize=250&page=3' #lik with the first 250 pdfs\n",
    "\n",
    "#URL = ''\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Find all <a> tags with href ending in .pdf (case insensitive)\n",
    "#Using XPath to find all anchor (<a>) tags whose href ends with .pdf, case-insensitive.\n",
    "pdf_links = driver.find_elements(By.XPATH, \"//a[translate(substring(@href, string-length(@href)-3), 'PDF', 'pdf')='.pdf']\")\n",
    "\n",
    "# Prepare PDF URLs (handle possible relative paths)\n",
    "base_url = driver.current_url\n",
    "downloaded = 0\n",
    "\n",
    "\n",
    "\n",
    "for link in pdf_links:\n",
    "    href = link.get_attribute('href')\n",
    "    if not href:\n",
    "        continue\n",
    "    # Resolve relative URLs\n",
    "    href_abs = urllib.parse.urljoin(base_url, href)\n",
    "    driver.get(href_abs)    #Navigating to the PDF link to trigger Chrome‚Äôs auto-download.\n",
    "    downloaded += 1\n",
    "    if downloaded >=250:   #Stops after downloading 250 files.\n",
    "            break\n",
    "    time.sleep(7) # Give browser time to download,Waits 2 seconds between downloads to avoid overlap.\n",
    "\n",
    "print(f\"Downloaded {downloaded} PDFs to {download_dir}\")  #Prints the number of PDFs downloaded and closes the browser.\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Show full content of each column (no truncation)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# # Show full rows and columns if needed\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pdf files from the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2017-10-02-pt1-PgE1299-6.pdf</td>\n",
       "      <td>October 2, 2017</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî Extensions of Remarks E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-1997-07-28-pt1-PgH5909.pdf</td>\n",
       "      <td>July 28, 1997</td>\n",
       "      <td>CONGRESSIONAL RECORD √êHOUSE H5909 July 28, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2025-05-15-pt1-PgS2939-3.pdf</td>\n",
       "      <td>May 15, 2025</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S2939 May 15, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2025-02-11-pt1-PgS826-2.pdf</td>\n",
       "      <td>February 11, 2025</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S826 February 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2018-10-05-pt1-PgS6628-2.pdf</td>\n",
       "      <td>October 5, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S6628 October 5,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename               date  \\\n",
       "0  CREC-2017-10-02-pt1-PgE1299-6.pdf    October 2, 2017   \n",
       "1    CREC-1997-07-28-pt1-PgH5909.pdf      July 28, 1997   \n",
       "2  CREC-2025-05-15-pt1-PgS2939-3.pdf       May 15, 2025   \n",
       "3   CREC-2025-02-11-pt1-PgS826-2.pdf  February 11, 2025   \n",
       "4  CREC-2018-10-05-pt1-PgS6628-2.pdf    October 5, 2018   \n",
       "\n",
       "                                                Text  \n",
       "0  CONGRESSIONAL RECORD ‚Äî Extensions of Remarks E...  \n",
       "1  CONGRESSIONAL RECORD √êHOUSE H5909 July 28, 199...  \n",
       "2  CONGRESSIONAL RECORD ‚Äî SENATE S2939 May 15, 20...  \n",
       "3  CONGRESSIONAL RECORD ‚Äî SENATE S826 February 11...  \n",
       "4  CONGRESSIONAL RECORD ‚Äî SENATE S6628 October 5,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path               # For easy handling of file paths\n",
    "from PyPDF2 import PdfReader           # To read and extract text from PDF files\n",
    "import re                              # For regular expressions (pattern matching)\n",
    "import pandas as pd                    # For working with data in tables (DataFrame)\n",
    "\n",
    "\n",
    "# Directory containing PDFs\n",
    "pdf_dir = Path(\"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/CONGRESS_DATA\")\n",
    "\n",
    "# Function to extract date and text\n",
    "def extract_date_and_text(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(str(pdf_path))  # Open the PDF using PyPDF2\n",
    "        full_text = \"\"                     # Initialize empty string to hold full PDF text\n",
    "        for page in reader.pages:          # Loop through all pages in the PDF\n",
    "            page_text = page.extract_text()   # Extract text from page\n",
    "            if page_text:\n",
    "                full_text += page_text + \"\\n\"   # Add page text to full text\n",
    "\n",
    "        # Extract date (e.g., May 19, 1998)\n",
    "        #Looking for a date format like May 19, 1998 using regex and stores it. If none is found, it uses \"Unknown Date\"\n",
    "        date_match = re.search(r'([A-Z][a-z]+ \\d{1,2}, \\d{4})', full_text)\n",
    "        date = date_match.group(0) if date_match else \"Unknown Date\"\n",
    "\n",
    "        # Clean out headers\n",
    "        #RemovING repeated headers like CONGRESSIONAL RECORD ‚Äî HOUSE that clutter the PDF content.\n",
    "        cleaned_text = re.sub(r'CONGRESSIONAL RECORD\\s+‚Äî\\s+HOUSE.*?\\n', '', full_text, flags=re.DOTALL)\n",
    "\n",
    "        #Return the result as a dictionary\n",
    "\n",
    "        return {\n",
    "            \"filename\": pdf_path.name,   # Just the file name (not full path)\n",
    "            \"date\": date,     # Extracting the date\n",
    "            \"Text\": cleaned_text.strip()  # Cleaned text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"date\": \"Error\",\n",
    "            \"Text\": f\"Error reading file: {e}\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Extract from all PDFs in folder\n",
    "records = []\n",
    "for pdf_file in pdf_dir.glob(\"*.pdf\"):   # Loop through all PDF files in the folder\n",
    "    result = extract_date_and_text(pdf_file)   # Runing the extraction function\n",
    "    records.append(result)    # Save the result in a list\n",
    "\n",
    "\n",
    "\n",
    "# Store in DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Preview and optionally save\n",
    "df.head()\n",
    "#df.to_csv(\"congress_pdf_texts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2017-10-02-pt1-PgE1299-6.pdf</td>\n",
       "      <td>October 2, 2017</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî Extensions of Remarks E...</td>\n",
       "      <td>congressional record extensions of remarks e o...</td>\n",
       "      <td>congressional record extensions of remarks e12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-1997-07-28-pt1-PgH5909.pdf</td>\n",
       "      <td>July 28, 1997</td>\n",
       "      <td>CONGRESSIONAL RECORD √êHOUSE H5909 July 28, 199...</td>\n",
       "      <td>congressional record house h july out in the p...</td>\n",
       "      <td>congressional record √∞house h5909 july 28 1997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2025-05-15-pt1-PgS2939-3.pdf</td>\n",
       "      <td>May 15, 2025</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S2939 May 15, 20...</td>\n",
       "      <td>congressional record senate s may bent on redu...</td>\n",
       "      <td>congressional record senate s2939 may 15 2025 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2025-02-11-pt1-PgS826-2.pdf</td>\n",
       "      <td>February 11, 2025</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S826 February 11...</td>\n",
       "      <td>congressional record senate s february times t...</td>\n",
       "      <td>congressional record senate s826 february 11 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2018-10-05-pt1-PgS6628-2.pdf</td>\n",
       "      <td>October 5, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S6628 October 5,...</td>\n",
       "      <td>congressional record senate s october in case ...</td>\n",
       "      <td>congressional record senate s6628 october 5 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename               date  \\\n",
       "0  CREC-2017-10-02-pt1-PgE1299-6.pdf    October 2, 2017   \n",
       "1    CREC-1997-07-28-pt1-PgH5909.pdf      July 28, 1997   \n",
       "2  CREC-2025-05-15-pt1-PgS2939-3.pdf       May 15, 2025   \n",
       "3   CREC-2025-02-11-pt1-PgS826-2.pdf  February 11, 2025   \n",
       "4  CREC-2018-10-05-pt1-PgS6628-2.pdf    October 5, 2018   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CONGRESSIONAL RECORD ‚Äî Extensions of Remarks E...   \n",
       "1  CONGRESSIONAL RECORD √êHOUSE H5909 July 28, 199...   \n",
       "2  CONGRESSIONAL RECORD ‚Äî SENATE S2939 May 15, 20...   \n",
       "3  CONGRESSIONAL RECORD ‚Äî SENATE S826 February 11...   \n",
       "4  CONGRESSIONAL RECORD ‚Äî SENATE S6628 October 5,...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  congressional record extensions of remarks e o...   \n",
       "1  congressional record house h july out in the p...   \n",
       "2  congressional record senate s may bent on redu...   \n",
       "3  congressional record senate s february times t...   \n",
       "4  congressional record senate s october in case ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  congressional record extensions of remarks e12...  \n",
       "1  congressional record √∞house h5909 july 28 1997...  \n",
       "2  congressional record senate s2939 may 15 2025 ...  \n",
       "3  congressional record senate s826 february 11 2...  \n",
       "4  congressional record senate s6628 october 5 20...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean and Preprocess the Text\n",
    "#Using basic NLP cleaning (e.g., lowercasing, removing stopwords, punctuation)\n",
    "\n",
    "\n",
    "# Define a function to clean the text without removing stopwords\n",
    "def clean_text(text):\n",
    "    # Replace multiple whitespace characters (spaces, tabs, newlines) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Replace all non-word characters (punctuation, symbols) with a space\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    \n",
    "    # Convert to lowercase for uniformity\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'text' column and create a new 'cleaned_text' column\n",
    "\n",
    "\n",
    "# Sample text data (replace with your actual DataFrame)\n",
    "df['cleaned_text'] = df['Text'].apply(lambda x: re.sub(r'[^A-Za-z\\s]', '', x))  # Removes numbers & symbols\n",
    "\n",
    "# Explanation:\n",
    "# [^A-Za-z\\s] ‚Üí keep only letters and spaces\n",
    "# re.sub ‚Üí substitute anything that doesn't match with ''\n",
    "\n",
    "# Optional: remove extra spaces and convert to lowercase\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip().lower())\n",
    "\n",
    "\n",
    "df['clean_text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Preview the first row\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target topics and keywords\n",
    "target_topics = {\n",
    "    \"Wildlife Conservation\": [\n",
    "        \"wildlife\", \"species protection\", \"biodiversity\", \n",
    "        \"conservation\", \"endangered species\", \"ecosystem\", \"preservation\"\n",
    "    ],\n",
    "    \"Habitat Loss\": [\n",
    "        \"habitat\", \"deforestation\", \"land loss\", \"urban sprawl\", \n",
    "        \"environmental degradation\", \"land use change\", \"ecosystem destruction\"\n",
    "    ],\n",
    "    \"Animal Rights\": [\n",
    "        \"animal rights\", \"animal welfare\", \"cruelty\", \"poaching\", \n",
    "        \"illegal hunting\", \"factory farming\", \"ethical treatment of animals\"\n",
    "    ],\n",
    "    \"Environmental Policy\": [\"legislation\", \"policy\", \"regulation\", \"law\", \"environmental governance\"]\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Weak Labels (Rule-based Labeling)\n",
    "def assign_topic(text):\n",
    "    for topic, keywords in target_topics.items():    # For each topic & its keywords\n",
    "        for kw in keywords:                         \n",
    "            if kw in text:                           # If any keyword is in the text\n",
    "                return topic                        # Assign that topic\n",
    "    return \"Other\"                                  # If no keywords match, label as \"Other\"\n",
    "#Apply Labeling\n",
    "df['label'] = df['clean_text'].apply(assign_topic) #Creating a new label column that assigns topics based on keyword matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2017-10-02-pt1-PgE1299-6.pdf</td>\n",
       "      <td>October 2, 2017</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî Extensions of Remarks E...</td>\n",
       "      <td>congressional record extensions of remarks e o...</td>\n",
       "      <td>congressional record extensions of remarks e12...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-1997-07-28-pt1-PgH5909.pdf</td>\n",
       "      <td>July 28, 1997</td>\n",
       "      <td>CONGRESSIONAL RECORD √êHOUSE H5909 July 28, 199...</td>\n",
       "      <td>congressional record house h july out in the p...</td>\n",
       "      <td>congressional record √∞house h5909 july 28 1997...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2025-05-15-pt1-PgS2939-3.pdf</td>\n",
       "      <td>May 15, 2025</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S2939 May 15, 20...</td>\n",
       "      <td>congressional record senate s may bent on redu...</td>\n",
       "      <td>congressional record senate s2939 may 15 2025 ...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2025-02-11-pt1-PgS826-2.pdf</td>\n",
       "      <td>February 11, 2025</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S826 February 11...</td>\n",
       "      <td>congressional record senate s february times t...</td>\n",
       "      <td>congressional record senate s826 february 11 2...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2018-10-05-pt1-PgS6628-2.pdf</td>\n",
       "      <td>October 5, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S6628 October 5,...</td>\n",
       "      <td>congressional record senate s october in case ...</td>\n",
       "      <td>congressional record senate s6628 october 5 20...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename               date  \\\n",
       "0  CREC-2017-10-02-pt1-PgE1299-6.pdf    October 2, 2017   \n",
       "1    CREC-1997-07-28-pt1-PgH5909.pdf      July 28, 1997   \n",
       "2  CREC-2025-05-15-pt1-PgS2939-3.pdf       May 15, 2025   \n",
       "3   CREC-2025-02-11-pt1-PgS826-2.pdf  February 11, 2025   \n",
       "4  CREC-2018-10-05-pt1-PgS6628-2.pdf    October 5, 2018   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CONGRESSIONAL RECORD ‚Äî Extensions of Remarks E...   \n",
       "1  CONGRESSIONAL RECORD √êHOUSE H5909 July 28, 199...   \n",
       "2  CONGRESSIONAL RECORD ‚Äî SENATE S2939 May 15, 20...   \n",
       "3  CONGRESSIONAL RECORD ‚Äî SENATE S826 February 11...   \n",
       "4  CONGRESSIONAL RECORD ‚Äî SENATE S6628 October 5,...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  congressional record extensions of remarks e o...   \n",
       "1  congressional record house h july out in the p...   \n",
       "2  congressional record senate s may bent on redu...   \n",
       "3  congressional record senate s february times t...   \n",
       "4  congressional record senate s october in case ...   \n",
       "\n",
       "                                          clean_text                  label  \n",
       "0  congressional record extensions of remarks e12...  Wildlife Conservation  \n",
       "1  congressional record √∞house h5909 july 28 1997...  Wildlife Conservation  \n",
       "2  congressional record senate s2939 may 15 2025 ...  Wildlife Conservation  \n",
       "3  congressional record senate s826 february 11 2...  Wildlife Conservation  \n",
       "4  congressional record senate s6628 october 5 20...  Wildlife Conservation  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this basic setup works better than your deep model, then the issue lies in how you're training DistilBERT or preparing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wildlife Conservation': 0}\n"
     ]
    }
   ],
   "source": [
    "#Encode the Labels\n",
    "#Converting text labels (strings) into numerical values so that machine learning models (like DistilBERT classifiers) can understand and process them\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "#Creating a dictionary showing the text-to-number mapping.\n",
    "label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Text'].tolist(),\n",
    "    df['label_encoded'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading DistilBERT and Tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "#BERT models like DistilBERT can't read raw text‚Äîthey need,Token IDs (integers representing words)\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#train_texts and val_texts are lists of your cleaned text samples.\n",
    "#truncation=True: If a text is too long (over 512 tokens), it gets cut off.\n",
    "#padding=True: Shorter texts are padded with zeros so all sequences are the same length.\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset Object\n",
    "\n",
    "import torch\n",
    "\n",
    "class ConservationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ConservationDataset(train_encodings, train_labels)\n",
    "val_dataset = ConservationDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mse_loss_out_mps: only defined for floating types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m\n\u001b[1;32m     15\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     16\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,                  \u001b[38;5;66;03m# Where to save the model/checkpoints\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,                      \u001b[38;5;66;03m# How many epochs to train\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,                        \u001b[38;5;66;03m# Log every 10 steps\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     29\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     30\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     31\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2242\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2243\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2244\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2245\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2246\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:1006\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m MSELoss()\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1006\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fct(logits\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fct(logits, labels)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3905\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3902\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3903\u001b[0m         )\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3906\u001b[0m         expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3907\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mse_loss_out_mps: only defined for floating types"
     ]
    }
   ],
   "source": [
    "#Fine-tune DistilBERT\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "#Importing libraries\n",
    "\n",
    "#DistilBertForSequenceClassification: A pre-built DistilBERT model with a classification head on top (dense layer for labels).\n",
    "\n",
    "#Trainer: Simplifies training and evaluation loops.\n",
    "\n",
    "#TrainingArguments: Defines training configurations (batch size, epochs, logging, saving..)\n",
    "\n",
    "\n",
    "#Loading Pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                  # Where to save the model/checkpoints\n",
    "    num_train_epochs=3,                      # How many epochs to train\n",
    "    per_device_train_batch_size=8,           # Batch size for training\n",
    "    per_device_eval_batch_size=8,            # Batch size for evaluation\n",
    "    evaluation_strategy=\"epoch\",             # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                   # Save model at each epoch\n",
    "    logging_dir='./logs',                    # Directory for logs\n",
    "    logging_steps=10,                        # Log every 10 steps\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.008101724088191986,\n",
       " 'eval_runtime': 12.3616,\n",
       " 'eval_samples_per_second': 1.618,\n",
       " 'eval_steps_per_second': 0.243,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()  #Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting topics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:13<00:00,  7.54it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:08<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predicted_topics = []\n",
    "\n",
    "device = torch.device(\"cpu\")  # Can switch to \"cuda\" if you have GPU\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Predict topics for df['Text']\n",
    "for text in tqdm(df['Text'], desc=\"Predicting topics\"):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    pred_class = outputs.logits.argmax(dim=1).item()\n",
    "    pred_label = le.inverse_transform([pred_class])[0]\n",
    "    predicted_topics.append(pred_label)\n",
    "\n",
    "df['Predicted_Topic'] = predicted_topics  # Add to DataFrame\n",
    "\n",
    "# üîß Define get_predictions function for evaluation\n",
    "def get_predictions(texts, labels, model, tokenizer, device):\n",
    "    preds = []\n",
    "    true = []\n",
    "\n",
    "    for text, label in tqdm(zip(texts, labels), total=len(texts), desc=\"Evaluating\"):\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        pred_class = outputs.logits.argmax(dim=1).item()\n",
    "        preds.append(pred_class)\n",
    "        true.append(label)\n",
    "\n",
    "    return preds, true\n",
    "\n",
    "# Evaluate on train and validation data\n",
    "train_preds, train_true = get_predictions(train_texts, train_labels, model, tokenizer, device)\n",
    "train_acc = accuracy_score(train_true, train_preds)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "val_preds, val_true = get_predictions(val_texts, val_labels, model, tokenizer, device)\n",
    "val_acc = accuracy_score(val_true, val_preds)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #performs inference (prediction) using your trained DistilBERT classification model on new text data.\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Initialize list to store predictions\n",
    "# predicted_topics = []\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device(\"cpu\")  # Forces the model to run on CPU (not GPU or MPS),but can be changed\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# for text in tqdm(df['Text'], desc=\"Predicting topics\"):\n",
    "#     inputs = tokenizer(\n",
    "#         text,\n",
    "#         return_tensors=\"pt\",\n",
    "#         truncation=True,\n",
    "#         padding=True\n",
    "#     )\n",
    "\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to CPU\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     pred_class = outputs.logits.argmax(dim=1).item()\n",
    "#     pred_label = le.inverse_transform([pred_class])[0]\n",
    "#     predicted_topics.append(pred_label)\n",
    "\n",
    "\n",
    "# # Get predictions on train data\n",
    "# train_preds, train_true = get_predictions(train_texts, train_labels, model, tokenizer, device)\n",
    "# train_acc = accuracy_score(train_true, train_preds)\n",
    "# print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# # Get predictions on validation/test data\n",
    "# val_preds, val_true = get_predictions(val_texts, val_labels, model, tokenizer, device)\n",
    "# val_acc = accuracy_score(val_true, val_preds)\n",
    "# print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        predicted_topics\n",
      "0  Wildlife Conservation\n",
      "1  Wildlife Conservation\n",
      "2  Wildlife Conservation\n",
      "3  Wildlife Conservation\n",
      "4  Wildlife Conservation\n"
     ]
    }
   ],
   "source": [
    "df_predicted = pd.DataFrame({'predicted_topics': predicted_topics})\n",
    "\n",
    "print(df_predicted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_topics     \n",
       "Wildlife Conservation    99\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "      <th>Predicted_Topic</th>\n",
       "      <th>predicted_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2022-06-14-pt1-PgH5533-4.pdf</td>\n",
       "      <td>June 14, 2022</td>\n",
       "      <td>is my duty to ensure that our voices \\nare hea...</td>\n",
       "      <td>is my duty to ensure that our voices are heard...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-2010-11-29-pt1-PgD1120-3.pdf</td>\n",
       "      <td>November 29, 2010</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî DAILY DIGEST D1120 Nove...</td>\n",
       "      <td>congressional record daily digest d1120 novemb...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2022-05-12-pt1-PgS2494-2.pdf</td>\n",
       "      <td>May 12, 2022</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S2494 May 12, 20...</td>\n",
       "      <td>congressional record senate s2494 may 12 2022 ...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2017-12-19-pt1-PgS8148-5.pdf</td>\n",
       "      <td>December 19, 2017</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S8148 December 1...</td>\n",
       "      <td>congressional record senate s8148 december 19 ...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2018-04-09-pt1-PgS2017.pdf</td>\n",
       "      <td>April 9, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S2017 April 9, 2...</td>\n",
       "      <td>congressional record senate s2017 april 9 2018...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>CREC-2010-07-14-pt1-PgS5848.pdf</td>\n",
       "      <td>July 14, 2010</td>\n",
       "      <td>CONGRESSIONAL RECORD ‚Äî SENATE S5848 July 14, 2...</td>\n",
       "      <td>congressional record senate s5848 july 14 2010...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CREC-1997-06-18-pt1-PgD632.pdf</td>\n",
       "      <td>June 18, 1997</td>\n",
       "      <td>CONGRESSIONAL RECORD √êDAILY DIGEST D632 June 1...</td>\n",
       "      <td>congressional record √∞daily digest d632 june 1...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CREC-1998-10-10-pt1-PgH10356-5.pdf</td>\n",
       "      <td>October 10, 1998</td>\n",
       "      <td>CONGRESSIONAL RECORD √êHOUSE H10356 October 10,...</td>\n",
       "      <td>congressional record √∞house h10356 october 10 ...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CREC-1997-11-05-pt1-PgS11763.pdf</td>\n",
       "      <td>November 5, 1997</td>\n",
       "      <td>CONGRESSIONAL RECORD √êSENATE S11763 November 5...</td>\n",
       "      <td>congressional record √∞senate s11763 november 5...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CREC-1998-05-20-pt1-PgS5217.pdf</td>\n",
       "      <td>May 20, 1998</td>\n",
       "      <td>CONGRESSIONAL RECORD √êSENATE S5217 May 20, 199...</td>\n",
       "      <td>congressional record √∞senate s5217 may 20 1998...</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "      <td>Wildlife Conservation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename               date  \\\n",
       "0    CREC-2022-06-14-pt1-PgH5533-4.pdf      June 14, 2022   \n",
       "1    CREC-2010-11-29-pt1-PgD1120-3.pdf  November 29, 2010   \n",
       "2    CREC-2022-05-12-pt1-PgS2494-2.pdf       May 12, 2022   \n",
       "3    CREC-2017-12-19-pt1-PgS8148-5.pdf  December 19, 2017   \n",
       "4      CREC-2018-04-09-pt1-PgS2017.pdf      April 9, 2018   \n",
       "..                                 ...                ...   \n",
       "94     CREC-2010-07-14-pt1-PgS5848.pdf      July 14, 2010   \n",
       "95      CREC-1997-06-18-pt1-PgD632.pdf      June 18, 1997   \n",
       "96  CREC-1998-10-10-pt1-PgH10356-5.pdf   October 10, 1998   \n",
       "97    CREC-1997-11-05-pt1-PgS11763.pdf   November 5, 1997   \n",
       "98     CREC-1998-05-20-pt1-PgS5217.pdf       May 20, 1998   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   is my duty to ensure that our voices \\nare hea...   \n",
       "1   CONGRESSIONAL RECORD ‚Äî DAILY DIGEST D1120 Nove...   \n",
       "2   CONGRESSIONAL RECORD ‚Äî SENATE S2494 May 12, 20...   \n",
       "3   CONGRESSIONAL RECORD ‚Äî SENATE S8148 December 1...   \n",
       "4   CONGRESSIONAL RECORD ‚Äî SENATE S2017 April 9, 2...   \n",
       "..                                                ...   \n",
       "94  CONGRESSIONAL RECORD ‚Äî SENATE S5848 July 14, 2...   \n",
       "95  CONGRESSIONAL RECORD √êDAILY DIGEST D632 June 1...   \n",
       "96  CONGRESSIONAL RECORD √êHOUSE H10356 October 10,...   \n",
       "97  CONGRESSIONAL RECORD √êSENATE S11763 November 5...   \n",
       "98  CONGRESSIONAL RECORD √êSENATE S5217 May 20, 199...   \n",
       "\n",
       "                                           clean_text                  label  \\\n",
       "0   is my duty to ensure that our voices are heard...  Wildlife Conservation   \n",
       "1   congressional record daily digest d1120 novemb...  Wildlife Conservation   \n",
       "2   congressional record senate s2494 may 12 2022 ...  Wildlife Conservation   \n",
       "3   congressional record senate s8148 december 19 ...  Wildlife Conservation   \n",
       "4   congressional record senate s2017 april 9 2018...  Wildlife Conservation   \n",
       "..                                                ...                    ...   \n",
       "94  congressional record senate s5848 july 14 2010...  Wildlife Conservation   \n",
       "95  congressional record √∞daily digest d632 june 1...  Wildlife Conservation   \n",
       "96  congressional record √∞house h10356 october 10 ...  Wildlife Conservation   \n",
       "97  congressional record √∞senate s11763 november 5...  Wildlife Conservation   \n",
       "98  congressional record √∞senate s5217 may 20 1998...  Wildlife Conservation   \n",
       "\n",
       "    label_encoded        Predicted_Topic        predicted_topic  \n",
       "0               1  Wildlife Conservation  Wildlife Conservation  \n",
       "1               1  Wildlife Conservation  Wildlife Conservation  \n",
       "2               1  Wildlife Conservation  Wildlife Conservation  \n",
       "3               1  Wildlife Conservation  Wildlife Conservation  \n",
       "4               1  Wildlife Conservation  Wildlife Conservation  \n",
       "..            ...                    ...                    ...  \n",
       "94              1  Wildlife Conservation  Wildlife Conservation  \n",
       "95              1  Wildlife Conservation  Wildlife Conservation  \n",
       "96              1  Wildlife Conservation  Wildlife Conservation  \n",
       "97              1  Wildlife Conservation  Wildlife Conservation  \n",
       "98              1  Wildlife Conservation  Wildlife Conservation  \n",
       "\n",
       "[99 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose this is your original dataframe\n",
    "# df['text'] contains your text data\n",
    "# predicted_topics is the list of predicted topics from your fine-tuned DistilBERT model\n",
    "\n",
    "# Create new dataframe with predicted topics\n",
    "df_result = df.copy()  # Keep original data\n",
    "\n",
    "df_result['predicted_topic'] = predicted_topics  # Add predictions\n",
    "\n",
    "# df_result['predicted_topic']\n",
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
