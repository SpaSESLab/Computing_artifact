{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Downloading Congressional documents about the endangered species act from a section of congressional Record of Daily Digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import service as chromeservice\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 207 PDFs to /Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\n"
     ]
    }
   ],
   "source": [
    "#Extracting pdf files for page 1\n",
    "from selenium import webdriver # Selenium is used for browser automation.\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # ChromeDriverManager automatically downloads the right version of ChromeDriver.\n",
    "from selenium.webdriver.common.by import By  # By is a locator strategy for finding elements (e.g., by tag, class, XPath).\n",
    "import time  # time is used to pause between downloads.\n",
    "import os\n",
    "import urllib.parse  # os and urllib.parse help with file paths and URL handling.\n",
    "import re\n",
    "\n",
    "\n",
    "# Setting my download path\n",
    "download_dir = \"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\"  #folder to put the downloads\n",
    "\n",
    "# function to wait for downloads to finish\n",
    "def wait_for_downloads(download_path, timeout=30):\n",
    "    seconds = 0\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        if seconds > timeout:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,    #Ensuring that Chrome doesn't try to preview PDFs but downloads them directly.\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=ChromeService(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "# URL to fetch\n",
    "#This opens the Congress.gov search results page for the keyword \"Endagered Species Act\" in the Congressional Record, maximizes the browser, and waits up to 5 seconds for elements to load.\n",
    "\n",
    "URL = 'https://www.congress.gov/quick-search/congressional-record?wordsPhrases=crArticleContent%3A%22endangered+species+act%22&congresses%5B0%5D=all&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionDailyDigest=on&representative%5B0%5D=&senator%5B0%5D=&pageSort=relevancy&pageSize=250#' #lik with the first 250 pdfs\n",
    "\n",
    "\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "pdf_links = driver.find_elements(By.XPATH, \"//a[translate(substring(@href, string-length(@href)-3), 'PDF', 'pdf')='.pdf']\")\n",
    "\n",
    "\n",
    "base_url = driver.current_url\n",
    "downloaded = 0\n",
    "\n",
    "\n",
    "\n",
    "for link in pdf_links:\n",
    "    href = link.get_attribute('href')\n",
    "    if not href:\n",
    "        continue\n",
    "    # Resolve relative URLs\n",
    "    href_abs = urllib.parse.urljoin(base_url, href)\n",
    "    driver.get(href_abs)    #Navigating to the PDF link to trigger Chrome’s auto-download.\n",
    "    downloaded += 1\n",
    "    if downloaded >=250:   #Stops after downloading 250 files.\n",
    "            break\n",
    "    time.sleep(7) \n",
    "\n",
    "print(f\"Downloaded {downloaded} PDFs to {download_dir}\")  #Printing the number of PDFs downloaded and closes the browser.\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 117 PDFs to /Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\n"
     ]
    }
   ],
   "source": [
    "#Extracting pdf fiels for page 2\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager  \n",
    "from selenium.webdriver.common.by import By \n",
    "import time  \n",
    "import os\n",
    "import urllib.parse  \n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "download_dir = \"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\"  \n",
    "\n",
    "\n",
    "def wait_for_downloads(download_path, timeout=30):\n",
    "    seconds = 0\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        if seconds > timeout:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,   \n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=ChromeService(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "URL = 'https://www.congress.gov/quick-search/congressional-record?wordsPhrases=crArticleContent%3A%22endangered+species+act%22&congresses%5B0%5D=all&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionDailyDigest=on&representative%5B0%5D=&senator%5B0%5D=&pageSort=relevancy&pageSize=250&page=2' #lik with the first 250 pdfs\n",
    "\n",
    "\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "pdf_links = driver.find_elements(By.XPATH, \"//a[translate(substring(@href, string-length(@href)-3), 'PDF', 'pdf')='.pdf']\")\n",
    "\n",
    "\n",
    "base_url = driver.current_url\n",
    "downloaded = 0\n",
    "\n",
    "\n",
    "\n",
    "for link in pdf_links:\n",
    "    href = link.get_attribute('href')\n",
    "    if not href:\n",
    "        continue\n",
    "    \n",
    "    href_abs = urllib.parse.urljoin(base_url, href)\n",
    "    driver.get(href_abs)    \n",
    "    downloaded += 1\n",
    "    if downloaded >=250:   \n",
    "            break\n",
    "    time.sleep(7) \n",
    "\n",
    "print(f\"Downloaded {downloaded} PDFs to {download_dir}\") \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting pdf fiels for page 3\n",
    "from selenium import webdriver # Selenium is used for browser automation.\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # ChromeDriverManager automatically downloads the right version of ChromeDriver.\n",
    "from selenium.webdriver.common.by import By  # By is a locator strategy for finding elements (e.g., by tag, class, XPath).\n",
    "import time  # time is used to pause between downloads.\n",
    "import os\n",
    "import urllib.parse  # os and urllib.parse help with file paths and URL handling.\n",
    "import re\n",
    "\n",
    "\n",
    "# Set your download path\n",
    "download_dir = \"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\"  #folder to put the downloads\n",
    "\n",
    "# Define function to wait for downloads to finish\n",
    "def wait_for_downloads(download_path, timeout=30):\n",
    "    seconds = 0\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        if seconds > timeout:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,    #Ensures that Chrome doesn't try to preview PDFs but downloads them directly.\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=ChromeService(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "# URL to fetch\n",
    "#This opens the Congress.gov search results page for the keyword \"Endagered Species Act\" in the Congressional Record, maximizes the browser, and waits up to 5 seconds for elements to load.\n",
    "\n",
    "URL = 'https://www.congress.gov/quick-search/congressional-record?wordsPhrases=crArticleContent%3A%22endangered+species+act%22&congresses%5B0%5D=all&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionDailyDigest=on&representative%5B0%5D=&senator%5B0%5D=&pageSort=relevancy&pageSize=250&page=3' #lik with the first 250 pdfs\n",
    "\n",
    "#URL = ''\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Find all <a> tags with href ending in .pdf (case insensitive)\n",
    "#Using XPath to find all anchor (<a>) tags whose href ends with .pdf, case-insensitive.\n",
    "pdf_links = driver.find_elements(By.XPATH, \"//a[translate(substring(@href, string-length(@href)-3), 'PDF', 'pdf')='.pdf']\")\n",
    "\n",
    "# Prepare PDF URLs (handle possible relative paths)\n",
    "base_url = driver.current_url\n",
    "downloaded = 0\n",
    "\n",
    "\n",
    "\n",
    "for link in pdf_links:\n",
    "    href = link.get_attribute('href')\n",
    "    if not href:\n",
    "        continue\n",
    "    # Resolve relative URLs\n",
    "    href_abs = urllib.parse.urljoin(base_url, href)\n",
    "    driver.get(href_abs)    #Navigating to the PDF link to trigger Chrome’s auto-download.\n",
    "    downloaded += 1\n",
    "    if downloaded >=250:   #Stops after downloading 250 files.\n",
    "            break\n",
    "    time.sleep(7) # Give browser time to download,Waits 2 seconds between downloads to avoid overlap.\n",
    "\n",
    "print(f\"Downloaded {downloaded} PDFs to {download_dir}\")  #Prints the number of PDFs downloaded and closes the browser.\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
