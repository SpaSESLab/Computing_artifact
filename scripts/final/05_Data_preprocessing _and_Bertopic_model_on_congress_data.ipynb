{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path               # For easy handling of file paths\n",
    "from PyPDF2 import PdfReader           # To read and extract text from PDF files\n",
    "import re                              # For regular expressions (pattern matching)\n",
    "import pandas as pd   \n",
    "\n",
    "\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf  # if you are using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Illegal character in Name Object (b'/\\x84\\xf8\\x88\\r\\xf77\\xc7\\x01\\x8b\\xf8\\x96\\x03\\xf8\\x96\\xf7s\\x15\\xfc\\x88\\x06}O\\x05\\xf8\\x88\\x06\\t\\x0e\\x94\\xf8')\n",
      "Illegal character in Name Object (b'/\\x84\\xf8\\x88\\r\\xf77\\xc7\\x01\\x8b\\xf8\\x96\\x03\\xf8\\x96\\xf7s\\x15\\xfc\\x88\\x06}O\\x05\\xf8\\x88\\x06\\t\\x0e\\x94\\xf8')\n",
      "Illegal character in Name Object (b'/\\x84\\xf8\\x88\\r\\xf77\\xc7\\x01\\x8b\\xf8\\x96\\x03\\xf8\\x96\\xf7s\\x15\\xfc\\x88\\x06}O\\x05\\xf8\\x88\\x06\\t\\x0e\\x94\\xf8')\n",
      "Illegal character in Name Object (b'/\\x98\\xa0\\xbe\\x8b\\xa2\\x08\\x92\\x89\\x8f\\x85\\x1e\\x7f\\x8b\\x89vy\\x86\\x08Y}L\\x96Mx\\x08j\\x81Y\\\\\\x8b')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path               # For easy handling of file paths\n",
    "from PyPDF2 import PdfReader           # To read and extract text from PDF files\n",
    "# Directory containing PDFs\n",
    "pdf_dir = Path(\"/Users/agnesnamyalo/Desktop/RESEARCH/DATA/Artifact_files\")\n",
    "\n",
    "\n",
    "# Function to extract date and text\n",
    "def extract_date_and_text(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(str(pdf_path))  # Open the PDF using PyPDF2\n",
    "        full_text = \"\"                     # Initialize empty string to hold full PDF text\n",
    "        for page in reader.pages:          # Loop through all pages in the PDF\n",
    "            page_text = page.extract_text()   # Extract text from page\n",
    "            if page_text:\n",
    "                full_text += page_text + \"\\n\"   # Add page text to full text\n",
    "\n",
    "        # Extract date (e.g., May 19, 1998)\n",
    "        #Looking for a date format like May 19, 1998 using regex and stores it. If none is found, it uses \"Unknown Date\"\n",
    "        date_match = re.search(r'([A-Z][a-z]+ \\d{1,2}, \\d{4})', full_text)\n",
    "        date = date_match.group(0) if date_match else \"Unknown Date\"\n",
    "\n",
    "        # Clean out headers\n",
    "        #RemovING repeated headers like CONGRESSIONAL RECORD — HOUSE that clutter the PDF content.\n",
    "        cleaned_text = re.sub(r'CONGRESSIONAL RECORD\\s+—\\s+HOUSE.*?\\n', '', full_text, flags=re.DOTALL)\n",
    "\n",
    "        #Return the result as a dictionary\n",
    "\n",
    "        return {\n",
    "            \"filename\": pdf_path.name,   # Just the file name (not full path)\n",
    "            \"date\": date,     # Extracting the date\n",
    "            \"Text\": cleaned_text.strip()  # Cleaned text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"date\": \"Error\",\n",
    "            \"Text\": f\"Error reading file: {e}\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Extract from all PDFs in folder\n",
    "records = []\n",
    "for pdf_file in pdf_dir.glob(\"*.pdf\"):   # Loop through all PDF files in the folder\n",
    "    result = extract_date_and_text(pdf_file)   # Runing the extraction function\n",
    "    records.append(result)    # Save the result in a list\n",
    "\n",
    "\n",
    "\n",
    "# Store in DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Preview and optionally save\n",
    "#df.head()\n",
    "#df.to_csv(\"congress_pdf_texts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2005-07-12-pt1-PgD739-2.pdf</td>\n",
       "      <td>July 12, 2005</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-2018-11-13-pt1-PgD1164.pdf</td>\n",
       "      <td>November 13, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2006-03-09-pt1-PgD217-2.pdf</td>\n",
       "      <td>March 9, 2006</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2002-06-18-pt1-PgD636.pdf</td>\n",
       "      <td>June 18, 2002</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2014-03-25-pt1-PgD309-2.pdf</td>\n",
       "      <td>March 25, 2014</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename               date  \\\n",
       "0  CREC-2005-07-12-pt1-PgD739-2.pdf      July 12, 2005   \n",
       "1   CREC-2018-11-13-pt1-PgD1164.pdf  November 13, 2018   \n",
       "2  CREC-2006-03-09-pt1-PgD217-2.pdf      March 9, 2006   \n",
       "3    CREC-2002-06-18-pt1-PgD636.pdf      June 18, 2002   \n",
       "4  CREC-2014-03-25-pt1-PgD309-2.pdf     March 25, 2014   \n",
       "\n",
       "                                                Text  \n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...  \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...  \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...  \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...  \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your STM keywords\n",
    "#stm_words_df = pd.read_csv(\"news_stm_topic_keywords_with_weights.csv\")\n",
    "#stm_keywords = set(w.lower() for w in stm_words_df['keyword'])\n",
    "\n",
    "\n",
    "# Load your STM keywords\n",
    "stm_words_df = pd.read_csv(\"news_stm_topic_keywords_with_weights.csv\")\n",
    "stm_keywords = set(\n",
    "    w.lower() for w in stm_words_df['keyword']\n",
    "    if w.lower() not in {'a.m.', 'p.m.'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plant', 'habitat', 'river', 'school', 'timber', 'u.s.', 'water', 'animal', 'eagle', 'state', 'federal', 'office', 'threaten', 'mountain', 'court', 'california', 'agency', 'otter', 'vote', 'farmer', 'people', 'republican', 'subc', 'bald', 'tree', 'turtle', 'case', 'bear', 'hunt', 'work', 'protect', 'congress', 'butterfly', 'environmental', 'clinton', 'list', 'kill', 'bird', 'administration', 'wildlife', 'time', 'oil', 'president', 'owl', 'industry', 'fire', 'condor', 'island', 'service', 'breed', 'law', 'house', 'land', 'wolf', 'salmon', 'forest', 'protection', 'county', 'plan', 'american', 'whale', 'log', 'marine', 'senate', 'make', 'park', 'year', 'bill', 'area', 'sea', 'grizzly', 'fish', 'dam', 'nest'}\n"
     ]
    }
   ],
   "source": [
    "print(stm_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lines_with_keywords(text, keywords):\n",
    "    lines = text.split('\\n')\n",
    "    filtered = [line for line in lines if any(keyword in line.lower() for keyword in keywords)]\n",
    "    return \"\\n\".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the DataFrame\n",
    "df['filtered_lines'] = df['Text'].apply(lambda x: filter_lines_with_keywords(x, stm_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>filtered_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2005-07-12-pt1-PgD739-2.pdf</td>\n",
       "      <td>July 12, 2005</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-2018-11-13-pt1-PgD1164.pdf</td>\n",
       "      <td>November 13, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2006-03-09-pt1-PgD217-2.pdf</td>\n",
       "      <td>March 9, 2006</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2002-06-18-pt1-PgD636.pdf</td>\n",
       "      <td>June 18, 2002</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2014-03-25-pt1-PgD309-2.pdf</td>\n",
       "      <td>March 25, 2014</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename               date  \\\n",
       "0  CREC-2005-07-12-pt1-PgD739-2.pdf      July 12, 2005   \n",
       "1   CREC-2018-11-13-pt1-PgD1164.pdf  November 13, 2018   \n",
       "2  CREC-2006-03-09-pt1-PgD217-2.pdf      March 9, 2006   \n",
       "3    CREC-2002-06-18-pt1-PgD636.pdf      June 18, 2002   \n",
       "4  CREC-2014-03-25-pt1-PgD309-2.pdf     March 25, 2014   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...   \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...   \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...   \n",
       "\n",
       "                                      filtered_lines  \n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...  \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...  \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...  \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...  \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plant', 'habitat', 'river', 'school', 'timber', 'u.s.', 'water', 'animal', 'eagle', 'state', 'federal', 'office', 'threaten', 'mountain', 'court', 'california', 'agency', 'otter', 'vote', 'farmer', 'people', 'republican', 'subc', 'bald', 'tree', 'turtle', 'case', 'bear', 'hunt', 'work', 'protect', 'congress', 'butterfly', 'environmental', 'clinton', 'list', 'kill', 'bird', 'administration', 'wildlife', 'time', 'oil', 'president', 'owl', 'industry', 'fire', 'condor', 'island', 'service', 'breed', 'law', 'house', 'land', 'wolf', 'salmon', 'forest', 'protection', 'county', 'plan', 'american', 'whale', 'log', 'marine', 'senate', 'make', 'park', 'year', 'bill', 'area', 'sea', 'grizzly', 'fish', 'dam', 'nest'}\n"
     ]
    }
   ],
   "source": [
    "print(stm_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lemmatize_remove_stopwords(texts):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    nlp.max_length = 2_000_000\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        cleaned_texts.append(\" \".join(tokens))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Usage example:\n",
    "df['cleaned_text'] = lemmatize_remove_stopwords(df['filtered_lines'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>filtered_lines</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2005-07-12-pt1-PgD739-2.pdf</td>\n",
       "      <td>July 12, 2005</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>congressional RECORD — DAILY DIGEST D739 July ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-2018-11-13-pt1-PgD1164.pdf</td>\n",
       "      <td>November 13, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>congressional record — DAILY DIGEST D1164 Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2006-03-09-pt1-PgD217-2.pdf</td>\n",
       "      <td>March 9, 2006</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "      <td>congressional RECORD — DAILY DIGEST D217 March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2002-06-18-pt1-PgD636.pdf</td>\n",
       "      <td>June 18, 2002</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "      <td>congressional RECORD — DAILY DIGEST d636 June ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2014-03-25-pt1-PgD309-2.pdf</td>\n",
       "      <td>March 25, 2014</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "      <td>congressional RECORD — DAILY DIGEST D309 March...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename               date  \\\n",
       "0  CREC-2005-07-12-pt1-PgD739-2.pdf      July 12, 2005   \n",
       "1   CREC-2018-11-13-pt1-PgD1164.pdf  November 13, 2018   \n",
       "2  CREC-2006-03-09-pt1-PgD217-2.pdf      March 9, 2006   \n",
       "3    CREC-2002-06-18-pt1-PgD636.pdf      June 18, 2002   \n",
       "4  CREC-2014-03-25-pt1-PgD309-2.pdf     March 25, 2014   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...   \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...   \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...   \n",
       "\n",
       "                                      filtered_lines  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...   \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...   \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  congressional RECORD — DAILY DIGEST D739 July ...  \n",
       "1  congressional record — DAILY DIGEST D1164 Nove...  \n",
       "2  congressional RECORD — DAILY DIGEST D217 March...  \n",
       "3  congressional RECORD — DAILY DIGEST d636 June ...  \n",
       "4  congressional RECORD — DAILY DIGEST D309 March...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all newline characters from the 'filtered_lines' column and save to 'cleaned_text'\n",
    "#df['cleaned_text'] = df['filtered_lines'].str.replace('\\n', ' ', regex=False)\n",
    "\n",
    "# Preview the DataFrame with the cleaned column\n",
    "#print(df[['filtered_lines', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_remove_newlines_lower(texts):  #lowering the text\n",
    "    return texts.str.replace('\\n', ' ', regex=False).str.lower() #removing sentence separators eg \\n\n",
    "\n",
    "df['cleaned_text'] = clean_remove_newlines_lower(df['filtered_lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with both letters and digits\n",
    "df['cleaned_text'] = df['cleaned_text'].str.replace(r'\\b\\w*\\d\\w*\\b', '', regex=True)\n",
    "\n",
    "# Optional: Remove extra whitespace\n",
    "df['cleaned_text'] = df['cleaned_text'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove patterns like E:\\CR\\FM\\D25MR4.REC dsk7sptvn1prod\n",
    "    text = re.sub(r'E:\\\\CR\\\\FM\\\\D\\d{2}[A-Z]{2}\\d\\.REC\\s+\\w+', '', text)\n",
    "    # Remove a.m., p.m., am, pm\n",
    "    text = re.sub(r'\\b(a\\.m\\.|p\\.m\\.|am|pm)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Remove weird artifacts like 'SH–216'\n",
    "    text = re.sub(r'\\bSH–216\\b', '', text)\n",
    "    # Remove non-alpha \"words\" (e.g., digits, single letters, dash-words)\n",
    "    text = re.sub(r'\\b[^a-zA-Z]+\\b', ' ', text)\n",
    "    # Remove any remaining fragments of form \"record\", \"hear\", \"hearing\", \"committee\", etc. (optional, for cleaner results)\n",
    "    text = re.sub(r'\\b(record|hear|hearing|committee|entitle|presentation|response|legislation|act|amend|title|estate|deceased|veteran|benefit|burial|claim|request|program|center|pilot|property|lease|taxpayer|individual|unemployment|bond|financing)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Remove multiple spaces/newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "# Apply the function to your DataFrame\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Optional: Define a set of valid two-letter English words to preserve ('an', 'in', 'is', 'on', etc.)\n",
    "valid_two_letter_words = set(['am', 'an', 'as', 'at', 'be', 'by', 'do', 'go', 'he', 'if', 'in', 'is', 'it', 'me', 'my', 'no', 'of', 'on', 'or', 'so', 'to', 'up', 'us', 'we'])\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove repetitive file/record headers and boilerplate codes\n",
    "    text = re.sub(r'\\b(E|SH|FM|REC|CR|D\\d{2}[A-Z]{2}\\d|dsk7sptvn1prod)\\b', '', text)\n",
    "    text = re.sub(r'E:\\\\CR\\\\FM\\\\D\\d{2}[A-Z]{2}\\d\\.REC(\\s+\\w+)*', '', text)\n",
    "    # Remove time marks (am/pm/a.m./p.m.)\n",
    "    text = re.sub(r'\\b(a\\.m\\.|p\\.m\\.|am|pm)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Remove digit-only \"words\" and punctuation noise\n",
    "    text = re.sub(r'\\b[\\d\\W]+\\b', ' ', text)\n",
    "    # Remove single letters (isolated)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    # Remove two-letter words not in valid set\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2 or word.lower() in valid_two_letter_words])\n",
    "    # Remove any extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply to DataFrame\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>filtered_lines</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2005-07-12-pt1-PgD739-2.pdf</td>\n",
       "      <td>July 12, 2005</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>congressional daily digest july new public law...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-2018-11-13-pt1-PgD1164.pdf</td>\n",
       "      <td>November 13, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>congressional daily digest november house of r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREC-2006-03-09-pt1-PgD217-2.pdf</td>\n",
       "      <td>March 9, 2006</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...</td>\n",
       "      <td>congressional daily digest march national repu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREC-2002-06-18-pt1-PgD636.pdf</td>\n",
       "      <td>June 18, 2002</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...</td>\n",
       "      <td>congressional daily digest june of the resolut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREC-2014-03-25-pt1-PgD309-2.pdf</td>\n",
       "      <td>March 25, 2014</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D309 March...</td>\n",
       "      <td>congressional daily digest march achieving cle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename               date  \\\n",
       "0  CREC-2005-07-12-pt1-PgD739-2.pdf      July 12, 2005   \n",
       "1   CREC-2018-11-13-pt1-PgD1164.pdf  November 13, 2018   \n",
       "2  CREC-2006-03-09-pt1-PgD217-2.pdf      March 9, 2006   \n",
       "3    CREC-2002-06-18-pt1-PgD636.pdf      June 18, 2002   \n",
       "4  CREC-2014-03-25-pt1-PgD309-2.pdf     March 25, 2014   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...   \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...   \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...   \n",
       "\n",
       "                                      filtered_lines  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "2  CONGRESSIONAL RECORD —DAILY DIGEST D217 March ...   \n",
       "3  CONGRESSIONAL RECORD —DAILY DIGEST D636 June 1...   \n",
       "4  CONGRESSIONAL RECORD — DAILY DIGEST D309 March...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  congressional daily digest july new public law...  \n",
       "1  congressional daily digest november house of r...  \n",
       "2  congressional daily digest march national repu...  \n",
       "3  congressional daily digest june of the resolut...  \n",
       "4  congressional daily digest march achieving cle...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents for topic modeling: 325\n"
     ]
    }
   ],
   "source": [
    "#Cleaning text column\n",
    "\n",
    "# Droping NaN or missing values\n",
    "df= df[df['cleaned_text'].notna()]\n",
    "\n",
    "# Dropping empty strings or strings with only spaces\n",
    "df= df[df['cleaned_text'].str.strip() != \"\"]\n",
    "\n",
    "# Dropping very short texts (less than 3 words, which confuse UMAP/HDBSCAN)\n",
    "df= df[df['cleaned_text'].str.split().str.len() > 2]\n",
    "\n",
    "# Convert to list of strings for BERTopic\n",
    "texts = df['cleaned_text'].astype(str).tolist()\n",
    "\n",
    "print(f\"Number of documents for topic modeling: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents for topic modeling: 325\n"
     ]
    }
   ],
   "source": [
    "#Cleaning text column\n",
    "\n",
    "# Droping NaN or missing values\n",
    "df= df[df['cleaned_text'].notna()]\n",
    "\n",
    "# Dropping empty strings or strings with only spaces\n",
    "df= df[df['cleaned_text'].str.strip() != \"\"]\n",
    "\n",
    "# Dropping very short texts (less than 3 words, which confuse UMAP/HDBSCAN)\n",
    "df= df[df['cleaned_text'].str.split().str.len() > 2]\n",
    "\n",
    "# Convert to list of strings for BERTopic\n",
    "texts = df['cleaned_text'].astype(str).tolist()\n",
    "\n",
    "print(f\"Number of documents for topic modeling: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the wildlife dataset to be used for STM\n",
    "df.to_csv('Artifact_congress_for_STMM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3602c57a0e714fd9ab0c7d69354077e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:43:47,069 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-13 11:43:47,490 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-13 11:43:47,490 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-13 11:43:47,504 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-13 11:43:47,508 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-13 11:43:47,658 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                        Name  \\\n",
      "0      0     58           0_subcommittee_states_senate_june   \n",
      "1      1     54        1_house_subcommittee_vote_department   \n",
      "2      2     51                2_senate_amendment_vote_yeas   \n",
      "3      3     47        3_subcommittee_states_united_federal   \n",
      "4      4     35         4_subcommittee_march_federal_fiscal   \n",
      "5      5     30       5_states_united_subcommittee_national   \n",
      "6      6     27  6_subcommittee_department_defense_security   \n",
      "7      7     23            7_subcommittee_march_year_fiscal   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [subcommittee, states, senate, june, united, h...   \n",
      "1  [house, subcommittee, vote, department, public...   \n",
      "2  [senate, amendment, vote, yeas, fund, neutral,...   \n",
      "3  [subcommittee, states, united, federal, house,...   \n",
      "4  [subcommittee, march, federal, fiscal, year, n...   \n",
      "5  [states, united, subcommittee, national, entit...   \n",
      "6  [subcommittee, department, defense, security, ...   \n",
      "7  [subcommittee, march, year, fiscal, hold, hear...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [senate measures introduced two bills were int...  \n",
      "1  [congressional daily digest march thority of t...  \n",
      "2  [senate agreed to con res budget resolution as...  \n",
      "3  [congressional daily digest september ment of ...  \n",
      "4  [congressional ðdaily digest march assistance ...  \n",
      "5  [congressional daily digest september reported...  \n",
      "6  [congressional daily digest march john bradley...  \n",
      "7  [congressional ðdaily digest march department ...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "\n",
    "# Set seeds for full reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure UMAP with fixed random state and params\n",
    "umap_model = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "\n",
    "# Encode documents using SentenceTransformer (texts is your list from df['cleaned_text'])\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# KMeans clustering with fixed random state\n",
    "kmeans_model = KMeans(n_clusters=9, random_state=42)\n",
    "predicted_topics = kmeans_model.fit_predict(embeddings)\n",
    "\n",
    "# Add stop word removal through CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Instantiate BERTopic with reproducible settings and custom vectorizer\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    nr_topics=None,\n",
    "    umap_model=umap_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit using precomputed clusters (offline mode)\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings=embeddings, y=predicted_topics)\n",
    "\n",
    "# Print topic information\n",
    "print(topic_model.get_topic_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Getting keywords for each topic (except -1, but in your case, there is no -1 so this check is redundant)\n",
    "topic_keywords = {}\n",
    "for topic_num in topic_model.get_topic_info()['Topic']:\n",
    "    keywords = topic_model.get_topic(topic_num)\n",
    "    keywords_str = ', '.join([kw[0] for kw in keywords])\n",
    "    topic_keywords[topic_num] = keywords_str\n",
    "\n",
    "# 2. Create the mapping DataFrame (this will be the same length/order as your df)\n",
    "df_mapped = pd.DataFrame({\n",
    "    'filename': df['filename'].values,\n",
    "    'date': df['date'].values,\n",
    "    'topics': topics\n",
    "})\n",
    "\n",
    "# Mapping the keywords for each topic\n",
    "df_mapped['keywords'] = df_mapped['topics'].map(topic_keywords)\n",
    "\n",
    "# Checking the number of rows\n",
    "print(df_mapped.shape)\n",
    "assert df_mapped.shape[0] == df.shape[0]\n",
    "\n",
    "\n",
    "df_mapped.head()\n",
    "df_mapped.to_csv(\"congress_BERTopic_topics_and_keywords.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             filename                date  topics  \\\n",
      "0    CREC-2005-07-12-pt1-PgD739-2.pdf       July 12, 2005       4   \n",
      "1     CREC-2018-11-13-pt1-PgD1164.pdf   November 13, 2018       3   \n",
      "2    CREC-2006-03-09-pt1-PgD217-2.pdf       March 9, 2006       4   \n",
      "3      CREC-2002-06-18-pt1-PgD636.pdf       June 18, 2002       1   \n",
      "4    CREC-2014-03-25-pt1-PgD309-2.pdf      March 25, 2014       6   \n",
      "..                                ...                 ...     ...   \n",
      "320    CREC-2014-02-27-pt1-PgD191.pdf   February 27, 2014       1   \n",
      "321  CREC-2002-06-21-pt1-PgD660-2.pdf       June 21, 2002       1   \n",
      "322    CREC-2003-05-14-pt2-PgD526.pdf        May 14, 2003       3   \n",
      "323    CREC-1996-09-17-pt1-PgD954.pdf  September 17, 1996       0   \n",
      "324    CREC-1995-08-03-pt2-PgD982.pdf      August 3, 1995       3   \n",
      "\n",
      "                                              keywords  \n",
      "0    subcommittee, march, federal, fiscal, year, na...  \n",
      "1    subcommittee, states, united, federal, house, ...  \n",
      "2    subcommittee, march, federal, fiscal, year, na...  \n",
      "3    house, subcommittee, vote, department, public,...  \n",
      "4    subcommittee, department, defense, security, u...  \n",
      "..                                                 ...  \n",
      "320  house, subcommittee, vote, department, public,...  \n",
      "321  house, subcommittee, vote, department, public,...  \n",
      "322  subcommittee, states, united, federal, house, ...  \n",
      "323  subcommittee, states, senate, june, united, ho...  \n",
      "324  subcommittee, states, united, federal, house, ...  \n",
      "\n",
      "[325 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "words = pd.read_csv(\"congress_BERTopic_topics_and_keywords.csv\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325, 4)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topics                                           keywords\n",
      "0       0  subcommittee, states, senate, june, united, ho...\n",
      "1       1  house, subcommittee, vote, department, public,...\n",
      "2       2  senate, amendment, vote, yeas, fund, neutral, ...\n",
      "3       3  subcommittee, states, united, federal, house, ...\n",
      "4       4  subcommittee, march, federal, fiscal, year, na...\n",
      "5       5  states, united, subcommittee, national, entitl...\n",
      "6       6  subcommittee, department, defense, security, u...\n",
      "7       7  subcommittee, march, year, fiscal, hold, heari...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get unique topic/keywords pairs\n",
    "keywords_df = df_mapped[['topics', 'keywords']].drop_duplicates().sort_values('topics').reset_index(drop=True)\n",
    "\n",
    "print(keywords_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df.to_csv(\"congress_bertopic_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the number of times a keyword appears\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Splitting keywords and flatten to one list\n",
    "all_keywords = []\n",
    "for kw_string in keywords_df['keywords']:\n",
    "    # Removiing spaces and split by comma, or use str.split(', ')\n",
    "    words = [kw.strip() for kw in kw_string.split(',')]\n",
    "    all_keywords.extend(words)\n",
    "\n",
    "# Count frequency of each keyword\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# Convert to DataFrame for easy handling/saving\n",
    "keyword_counts_df = pd.DataFrame(keyword_counts.items(), columns=['keyword', 'count']).sort_values('count', ascending=False)\n",
    "\n",
    "#print(keyword_counts_df)\n",
    "\n",
    "keyword_counts_df.to_csv('Bertopic_keyword_counts_for_congress.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    topic   keyword    weight\n",
      "75      7  hearings  0.041160\n",
      "76      7     april  0.039461\n",
      "77      7  national  0.032943\n",
      "78      7   rayburn  0.032658\n",
      "79      7   federal  0.028712\n"
     ]
    }
   ],
   "source": [
    "#Getting the weights of the keywords\n",
    "\n",
    "topic_nums = [topic for topic in topic_model.get_topic_info()['Topic']]\n",
    "\n",
    "# Gather all (topic, keyword, weight) rows\n",
    "records = []\n",
    "for topic_num in topic_nums:\n",
    "    keywords_with_weights = topic_model.get_topic(topic_num)\n",
    "    for word, weight in keywords_with_weights:\n",
    "        records.append({'topic': topic_num, 'keyword': word, 'weight': weight})\n",
    "\n",
    "# Create dataframe of keyword weights per topic\n",
    "keyword_weights_df = pd.DataFrame(records)\n",
    "\n",
    "# View results\n",
    "print(keyword_weights_df.tail())\n",
    "\n",
    "keyword_weights_df.to_csv('congress_bertopic_keywords_with_weights_01.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stm_words = pd.read_csv(\"congress_stm_topic_keywords_with_weights.csv\")\n",
    "#print(stm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting keywords in stm\n",
    "\n",
    "keyword_counts_stm = stm_words.groupby('keyword').size().reset_index(name='count')\n",
    "\n",
    "#print(keyword_counts_stm)\n",
    "\n",
    "keyword_counts_stm.to_csv('STM_keyword_counts_for_congress.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>Text</th>\n",
       "      <th>filtered_lines</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>gamma</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREC-2005-07-12-pt1-PgD739-2.pdf</td>\n",
       "      <td>July 12, 2005</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...</td>\n",
       "      <td>congressional daily digest july new public law...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.977161</td>\n",
       "      <td>subcommittee, united, states, entitled, security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREC-2018-11-13-pt1-PgD1164.pdf</td>\n",
       "      <td>November 13, 2018</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...</td>\n",
       "      <td>congressional daily digest november house of r...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.838841</td>\n",
       "      <td>states, united, senate, located, service</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename               date  \\\n",
       "0  CREC-2005-07-12-pt1-PgD739-2.pdf      July 12, 2005   \n",
       "1   CREC-2018-11-13-pt1-PgD1164.pdf  November 13, 2018   \n",
       "\n",
       "                                                Text  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "\n",
       "                                      filtered_lines  \\\n",
       "0  CONGRESSIONAL RECORD —DAILY DIGEST D739 July 1...   \n",
       "1  CONGRESSIONAL RECORD — DAILY DIGEST D1164 Nove...   \n",
       "\n",
       "                                        cleaned_text  document  topic  \\\n",
       "0  congressional daily digest july new public law...         1      3   \n",
       "1  congressional daily digest november house of r...         2      1   \n",
       "\n",
       "      gamma                                          keywords  \n",
       "0  0.977161  subcommittee, united, states, entitled, security  \n",
       "1  0.838841          states, united, senate, located, service  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STM_topics = pd.read_csv(\"congress_STM_topics_and_keywords.csv\")\n",
    "STM_topics.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
